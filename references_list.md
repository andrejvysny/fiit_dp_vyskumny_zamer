# List of references and short summary of each document

| ID | File name | Full citation | 1–3 sentence summary | Keywords | Relevance to thesis |
|----|-----------|--------------|----------------------|----------|---------------------|
| R01 | 1102.0676v1.pdf | Mukhopadhyay D. et al., “Architecture of a Scalable Dynamic Parallel WebCrawler with High Speed Downloadable Capability,” MSPT Workshop 2006 | Server-centric parallel crawler with seed-server routing prevents overlap, partitions by domain, and scales multithreaded downloads while enforcing politeness. | parallel crawler, seed server, scalability, URL registry, web search | Baseline architecture for high-throughput crawling to compare with AI-driven improvements. |
| R02 | 1133-3390-1-PB.pdf | Jeyapal A., Palanisamy G., “Multi-level Frontier based Topic-specific Crawler Design with Improved URL Ordering,” Computer and Information Science, 2008 | Multi-agent focused crawler using coordinator plus multi-level queue frontiers and URL ordering to tunnel toward relevant topics while avoiding duplicates. | focused crawling, multi-agent, URL frontier, relevance, de-duplication | Illustrates queueing and coordination strategies for topic-aware scraping. |
| R03 | 1406.5690v1.pdf | Gupta S. et al., “WebParF: A Web Partitioning Framework for Parallel Crawler,” IJCSE 2013 | Describes domain-based partitioning, dispatcher-classifier pipeline, and multithreaded downloaders; notes need for better classifiers and dispatch heuristics. | parallel crawler, domain partitioning, dispatcher, load balancing | Motivates partitioning and classifier design for scalable AI scrapers. |
| R04 | 1601.06919v1.pdf | Boldi P. et al., “BUbiNG: Massive Crawling for the Masses,” 2016 | Open-source distributed Java crawler achieving thousands of pages/sec with strict host/IP politeness, showing linear scaling with agents. | distributed crawler, open-source, politeness, scalability | Strong engineering baseline and metrics for evaluating new scraper performance. |
| R05 | 2201.03916v2.pdf | Parker-Holder J. et al., “Automated Reinforcement Learning (AutoRL): A Survey and Open Problems,” JAIR 2022 | Surveys automation across RL pipeline (search, tuning, architecture, data), highlighting interdependent components and open challenges. | AutoRL, hyperparameter search, pipeline automation, survey | Provides methodological grounding for automating scraping agents with RL. |
| R06 | 2202.04337v1.pdf | Yerushalmi R. et al., “Scenario-Assisted Deep Reinforcement Learning,” 2022 | Combines scenario-based specifications with DRL to enforce constraints; experiments show large drop in violations while keeping rewards similar. | constrained RL, scenario modeling, safety, constraints | Suggests adding rule-based constraints to AI scrapers to avoid unsafe actions. |
| R07 | 2209.12006v2.pdf | Finkelstein M. et al., “Explainable Reinforcement Learning via Model Transforms,” 2022 | Generates explanations by applying formal model transforms to RL agents; efficiently searches transform space and proposes human-in-the-loop extensions. | explainable RL, model transforms, transparency | Inspiration for making agent scraping decisions interpretable. |
| R08 | 2210.03945v2.pdf | Gur I. et al., “Understanding HTML with Large Language Models,” 2022 | Fine-tunes pretrained LLMs for HTML classification, description, and navigation; pretrained language knowledge transfers well, outperforming HTML-only training. | LLM, HTML understanding, web navigation, transfer learning | Supports using LLMs to reason over DOM/HTML in scrapers. |
| R09 | 2212.00253v1.pdf | Yin Q. et al., “Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox,” 2023 | Reviews distributed DRL methods and toolchains, discussing scalability, communication, self-play, and future directions for large-scale training. | distributed RL, toolbox, scalability, multi-agent | Guides distributed training setup for scalable scraping agents. |
| R10 | 2304.11960v4.pdf | Kuehn P. et al., “ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain,” 2023 | Dynamic pathing crawler classifies documents before following links, achieving ~52% relevant harvest for CTI sources; discusses limitations and future work. | focused crawler, CTI, BERT, relevance ranking | Domain-specific approach showing ML-based relevance filtering for scrapers. |
| R11 | 2305.05027v2.pdf | Vörös T. et al., “WebContent Filtering through Knowledge Distillation of Large Language Models,” 2023 | Fine-tunes LLM teacher and distills to 175× smaller student for URL categorization, improving accuracy by ~9% over prior methods. | LLM distillation, URL classification, content filtering | Demonstrates distillation to deploy lightweight classifiers within scrapers. |
| R12 | 2308.07107v2.pdf | Zhu Y. et al., “Large Language Models for Information Retrieval: A Survey,” 2023 | Surveys LLM use across IR components (query rewriting, retriever, reranker, reader) and outlines future research challenges. | LLM, IR, retrieval pipeline, survey | Context for integrating LLM-powered retrieval into scraping workflows. |
| R13 | 2312.03863v2.pdf | Wan Z. et al., “Efficient Large Language Models: A Survey,” 2023 | Taxonomizes efficiency techniques (compression, scheduling, data) for LLMs and highlights open efficiency challenges. | efficiency, LLM, compression, optimization | Guides efficiency strategies for deploying LLM components in scrapers. |
| R14 | 2404.07738v2.pdf | Baek J. et al., “ResearchAgent: Iterative Research Idea Generation over Scientific Literature with LLMs,” 2024 | LLM agents generate and iteratively refine research ideas using literature review and reviewer agents; ideas rated more creative/valid. | LLM agents, idea generation, iterative review | Illustrates agent collaboration patterns applicable to scraper planning. |
| R15 | 2404.09770v1.pdf | Thompson H., “Improved Methodology for Longitudinal Web Analytics using Common Crawl,” 2024 | Uses enhanced CC indexes (Last-Modified timestamps, segment ranking) to enable efficient longitudinal analysis with reduced storage/compute; notes anomalies. | Common Crawl, indexing, longitudinal analysis, efficiency | Techniques for handling large web corpora when evaluating scraper output. |
| R16 | 2404.12753v2.pdf | Huang W. et al., “AUTOSCRAPER: A Progressive Understanding Web Agent for Web Scraper Generation,” 2024 | Two-stage framework with progressive HTML understanding and synthesis to generate reusable site-specific scrapers; introduces executability metric; outperforms baselines. | LLM agents, scraper generation, progressive parsing, executability | Direct method for LLM-generated scrapers adaptable to dynamic sites. |
| R17 | 2405.14779v1.pdf | García-Romero C. et al., “Smart Bilingual Focused Crawling of Parallel Documents,” 2024 | Guides crawling toward parallel texts via URL language inference and parallel-link prediction, accelerating discovery of bilingual corpora. | focused crawl, parallel corpora, language ID, URL pairing | Relevant for multilingual data extraction and evaluation. |
| R18 | 2405.18687v1.pdf | “Advancing Household Robotics: Deep Interactive Reinforcement Learning for Efficient Training and Enhanced Performance,” 2024 | Reviews DeepRL for domestic robots and proposes five-sprint plan with interactive feedback and persistence mechanisms; outlines future work. | household robotics, DeepRL, interactive feedback, training plan | Shows structured RL methodology and sprint planning transferable to project planning. |
| R19 | 2406.04638v1.pdf | Kong X. et al., “Large Language Model-guided Document Selection,” 2024 | Uses prompted LLM grader distilled into classifier to prune 75% of web-crawl corpus; filtered data improves benchmarks with less compute. | data filtering, LLM grader, corpus pruning, quality | Informs training-data curation for scraper/evaluator models. |
| R20 | 2406.08246v1.pdf | Ahluwalia A., Wani S., “Leveraging Large Language Models for Web Scraping,” 2024 | Proposes RAG-style scraping pipeline combining pretrained LLM knowledge with retrievers; future work includes fine-tuning to reduce biases. | LLM, RAG, web scraping, data extraction | Conceptual recipe for LLM-assisted scraping relevant to prototype design. |
| R21 | 2407.07630v1.pdf | Perełkiewicz M., Poświata R., “Challenges with Massive Web-mined Corpora Used in LLM Pre-training,” 2024 | Reviews issues of noise, duplication, bias, and sensitive data in web-mined corpora; calls for robust cleaning and ethics. | web corpora, data quality, bias, ethics | Highlights risks and cleaning needs when building scraper datasets. |
| R22 | 2407.12170v1.pdf | Chang X. et al., “Neural Passage Quality Estimation for Static Pruning,” 2024 | Trains models to estimate query-agnostic passage quality for pruning; small models nearly match larger ones while shrinking indexes and costs. | passage quality, pruning, efficiency, static index | Approach for quality-based filtering of scraped content before indexing. |
| R23 | 2409.15441v1.pdf | Tang B., Shin K., “Steward: Natural Language Web Automation,” 2024 | LLM-powered web automation tool with HTML filtering, caching, and branch prediction to scale end-to-end web interactions. | web automation, LLM agent, caching, scalability | Provides automation patterns and optimizations for scraper agents. |
| R24 | 2412.03398v1.pdf | Chang Y. et al., “REDSTONE: Curating General, Code, Math, and QA Data for LLMs,” 2024 | Presents extraction/filtering pipeline over Common Crawl to build 3.48T-token domain datasets; shows effectiveness and plans future work. | data pipeline, Common Crawl, curation, domain datasets | Offers reusable pipeline ideas for building evaluation corpora from scraped data. |
| R25 | 2502.07056v1.pdf | Yu A. et al., “Autonomous Deep Agent,” 2025 | Hierarchical task DAG with recursive planner-executor and autonomous API/tool creation plus feedback refinement for complex tasks. | agent architecture, HTDAG, planning, tool creation | Architectural inspiration for orchestrating multi-step scraping workflows. |
| R26 | 2502.09913v1.pdf | Zhu Z. et al., “AutoS2earch: Unlocking the Reasoning Potential of Large Models for Web-based Source Search,” 2025 | Zero-shot source search combining visual-language translation and chain-of-thought reasoning, achieving 95–98% of human-AI collab performance. | source search, LLM reasoning, zero-shot, web apps | Shows LLM reasoning for locating sources, relevant to evaluation of scraped info. |
| R27 | 2502.10200v1.pdf | Shibata K., “Dynamic Reinforcement Learning for Actors,” 2025 | Controls system dynamics via sensitivity adjustment and reinforcement; experiments show benefits of SAL/SRL in changing environments. | dynamic RL, sensitivity control, chaos, adaptation | Alternative RL approach that may aid adaptive scraping agents. |
| R28 | 2503.09223v1.pdf | Tang T. et al., “LREF: A Novel LLM-based Relevance Framework for E-commerce Search,” 2025 | Framework selects high-quality labels, guides task-specific reasoning, and applies DPO debiasing to improve query-product relevance online/offline. | e-commerce search, LLM, de-biasing, relevance | Demonstrates LLM-guided relevance modeling applicable to ranking scraped results. |
| R29 | 2503.18102v1.pdf | Schmidgall S., Moor M., “AgentRxiv: Towards Collaborative Autonomous Research,” 2025 | Shared preprint server for LLM agent labs to iteratively improve methods; notes need for QA, ethics, and hallucination control. | agent collaboration, research sharing, quality control | Highlights collaborative workflows and oversight for agent-driven systems. |
| R30 | 2503.23350v1.pdf | Ning L. et al., “A Survey of WebAgents,” 2025 | Surveys AI WebAgents for automation tasks, covering models, tools, benchmarks, and open research issues. | WebAgents, survey, automation, benchmarks | Broad landscape of agentic web automation informing design choices. |
| R31 | 2504.03160v4.pdf | Zheng Y. et al., “DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments,” 2025 | RL-trained agents interact with live search engines end-to-end, outperforming prompt/RAG baselines; suggests dynamic parameter optimization. | deep research, RL, live web, search agent | Evidence that RL on real web improves task success for scraping-like agents. |
| R32 | 2504.06219v2.pdf | Fan D. et al., “Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs,” COLM 2025 | Defines Data Compliance Gap comparing robots.txt-compliant vs non-compliant corpora; finds compliant training can stay performant and recommends adherence. | data compliance, robots.txt, ethics, performance gap | Guidance for ethical scraping respecting site policies. |
| R33 | 2504.11011v1.pdf | Pezzuti F. et al., “Document Quality Scoring for Web Crawling,” 2025 | Dockerized neural quality scorer prioritized in crawl scheduling; early experiments show improved focus on high-quality pages. | quality scoring, crawling, neural scorer, prioritization | Practical module for quality-aware scraping pipelines. |
| R34 | 2506.16146v2.pdf | Pezzuti F. et al., “Neural Prioritisation for Web Crawling,” 2025 | Tests semantic quality-driven crawl policies vs BFS/oracle, improving early crawler and retriever effectiveness for natural-language queries. | crawl prioritization, semantic quality, policy | Supports quality-first scheduling for dynamic scrapers. |
| R35 | 2507.02592v1.pdf | Li K. et al., “WebSailor: Navigating Super-human Reasoning for Web Agent,” 2025 | Post-training pipeline with synthetic tasks, RFT cold start, and RL to reduce uncertainty in info-seeking; surpasses human and proprietary baselines. | web agent, RL, uncertainty reduction, synthetic data | Advanced training recipe for high-performing web agents. |
| R36 | 2508.19828v4.pdf | Yan S. et al., “Memory-R1: Enhancing LLM Agents to Manage and Utilize Memories via Reinforcement Learning,” 2025 | RL-trained memory manager decides add/update/delete operations, improving long-horizon QA with minimal data; shows benefits of better memory quality. | memory management, RL, long-horizon, LLM agent | Memory handling strategy for agents scraping multi-step sites. |
| R37 | 2509.13305v1.pdf | Li K. et al., “WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable RL,” 2025 | Builds SailorFog-QA-V2 dataset and dual-encoder training; open-source agent reaches proprietary-level deep research performance. | synthetic data, RL, web agent, deep search | Shows scaling path for open-source agents relevant to scraper training. |
| R38 | 2509.13309v2.pdf | Qiao Z. et al., “WebResearcher: Unleashing Unbounded Reasoning Capability in Long-Horizon Agents,” 2025 | IterResearch MDP with periodic report synthesis plus data engine; achieves SOTA on deep research benchmarks beyond proprietary systems. | long-horizon, iterative research, MDP, web agent | Framework for robust multi-stage reasoning over web sources. |
| R39 | 2509.13310v1.pdf | Su L. et al., “Scaling Agents via Continual Pre-training,” 2025 | Introduces agentic continual pretraining with synthesized planning/reasoning actions to improve alignment before post-training. | continual pretraining, agent alignment, synthetic actions | Suggests pretraining phase to boost agentic scraping behaviors. |
| R40 | 2510.24698v1.pdf | Li B. et al., “PARALLELMUSE: Agentic Parallel Thinking for Deep Information Seeking,” 2025 | Two-stage paradigm (partial rollout + hierarchical synthesis) reducing redundant rollouts and better aggregating long reasoning; improves performance with fewer resources. | parallel thinking, info seeking, efficiency, RL | Offers efficiency techniques for complex web tasks. |
| R41 | 2510.24699v1.pdf | Ye R. et al., “AgentFold: Long-Horizon Web Agents with Proactive Context Management,” 2025 | Proactive “folding” of context to avoid saturation; fold-generator data and training improve long-horizon navigation on BrowseComp. | context management, folding, long-horizon, web agent | Strategy to manage context windows when scraping deep sites. |
| R42 | 3637528.3671620.pdf | Lai H. et al., “AutoWebGLM: A Large Language Model-based Web Navigating Agent,” 2023 | Curriculum + self-sampling RL with AutoWebBench dataset yields strong bilingual web navigation; outlines contributions and performance. | web navigation, LLM, curriculum RL, benchmark | Baseline agent approach for automated browsing tasks. |
| R43 | Semantic_crawling_An_approach_based_on_Named_Entity_Recognition.pdf | Di Pietro G. et al., “Semantic Crawling: an Approach based on Named Entity Recognition,” ASONAM 2014 | LEA-focused crawler uses NER to fetch semantically relevant documents, normalizing text and reducing manual effort. | semantic crawling, NER, OSINT, focused crawl | Shows NER-driven semantic targeting useful for structured extraction. |
| R44 | applsci-10-03837-v2.pdf | Hernández J. et al., “A Semantic Focused Web Crawler Based on a Knowledge Representation Schema,” Applied Sciences 2020 | Builds knowledge-representation schema automatically from corpus to drive semantic focused crawling; shows promising filtering and proposes future work. | semantic crawl, KRS, ontology-lite, focused | Alternative semantic guidance mechanism for scraper targeting. |
| R45 | jcssp.2015.120.126.pdf | Jaganathan P., Karthikeyan T., “Highly Efficient Architecture for Scalable Focused Crawling Using Incremental Parallel Web Crawler,” JCSSP 2015 | Proposes incremental parallel focused crawler to reduce network load and maintain freshness via multi-objective URL distribution. | incremental crawl, focused crawler, freshness, parallelism | Techniques for incremental updates and freshness monitoring in scrapers. |
| R46 | 2025.acl-srw.19.pdf | Neveditsin N. et al., “Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes,” ACL SRW 2025 | Benchmarks small LMs generating JSON/YAML/XML for clinical extraction; JSON most parseable, prompting improves robustness but long notes degrade structure. | structured extraction, SLM, clinical notes, serialization | Practical guidance on serialization formats and prompts for reliable scraped data output. |
| R47 | 2107.06955.pdfv1.pdf | Aghajanyan A. et al., “HTLM: Hyper-Text Pre-Training and Prompting of Language Models,” 2021 | Pretrains BART-style denoising on simplified HTML to leverage markup semantics, boosting zero-shot/finetuned performance on classification and summarization. | HTML LM, pretraining, prompting, web text | Shows benefits of keeping HTML signals when training agents for web tasks. |
| R48 | 2201.10608.pdfv1.pdf | Deng X. et al., “DOM-LM: Learning Generalizable Representations for HTML Documents,” 2022 | Transformer encodes text plus DOM tree with self-supervised objectives, outperforming baselines on attribute extraction, OIE, and QA over webpages. | DOM encoding, representation learning, HTML | Strong baseline for HTML-aware embeddings in scraper pipelines. |
| R49 | 2201.10608.pdfv1 (1).pdf | Deng X. et al., “DOM-LM: Learning Generalizable Representations for HTML Documents,” 2022 | Duplicate of R48 (file copy). | duplicate | File duplicate of DOM-LM. |
| R50 | 2202.00217.pdfv1.pdf | Wang Q. et al., “WebFormer: The Web-page Transformer for Structure Information Extraction,” 2022 | Introduces HTML tokens with graph attention and layout-aware attention patterns to extract structured fields; beats prior methods on SWDE and Common Crawl. | web extraction, transformer, HTML tokens, layout | Architecture for leveraging DOM layout in structured scraping. |
| R51 | 2202.00217.pdfv1 (1).pdf | Wang Q. et al., “WebFormer: The Web-page Transformer for Structure Information Extraction,” 2022 | Duplicate of R50 (file copy). | duplicate | File duplicate of WebFormer. |
| R52 | 2210.03347v2.pdf | Lee K. et al., “Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding,” ICML 2023 | Pretrains image-to-text model to emit simplified HTML from masked screenshots; variable-resolution inputs and prompt rendering yield SOTA across document/UI/image tasks. | screenshot parsing, VLU, HTML generation, pretraining | Useful for agents converting web UIs to structured representations. |
| R53 | 2307.12856v4.pdf | Gur I. et al., “WebAgent: A Real-World Web Agent with Planning, Long Context Understanding, and Program Synthesis,” ICLR 2024 | Modular agent using Flan-U-PaLM for grounded code and HTML-T5 for planning/summarization; decomposes tasks, summarizes long HTML, and executes Python actions, improving real-site success by >50%. | web agent, planning, long context, HTML-T5 | Reference architecture for robust multi-step scraping agents. |
| R54 | 2309.11042.pdfv1.pdf | Xie Y. et al., “Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters,” 2023 | ALTER system adds mixture-of-task adapters with two-stage training to small LMs (<1B) for simultaneous NLP tasks, capturing cross-task knowledge efficiently. | adapters, multi-task, small LM, efficiency | Strategy for lightweight multi-task capability in scraper-side models. |
| R55 | 2310.01119.pdfv2.pdf | Kaddour J., Liu Q., “Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models,” 2023 | Fine-tunes teacher LLMs to generate/label synthetic data, boosting downstream small-model performance on classification and generation with minimal real labels. | synthetic data, low-resource, fine-tuning | Supports data augmentation for scraper-related classifiers under scarce labels. |
| R56 | 2312.15230v3.pdf | Zimmer M. et al., “PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs,” 2024 | Shows retraining only 0.01–0.05% parameters (LoRA-style) recovers pruned LLM performance; introduces mergeable adapters improving high-sparsity recovery. | pruning, LoRA, efficiency, sparsity | Suggests compute-light recovery steps for compressed agent models. |
| R57 | 2402.04437.pdfv5.pdf | Wu H. et al., “Learning to Extract Structured Entities Using Language Models,” 2024 | Reframes IE as entity-centric with AESOP metric and proposes multi-stage MuSEE model combining LM stages; outperforms baselines in structured entity extraction. | IE, entity extraction, metrics, multi-stage LM | Provides evaluation metric and pipeline for structured data extraction from scraped text. |
| R58 | 2402.14129.pdfv1.pdf | Hong Z. et al., “Combining Language and Graph Models for Semi-structured Information Extraction on the Web,” 2024 | GraphScholarBERT fuses graph and language encoders to zero-shot extract relations from semi-structured pages, improving F1 up to 34.8% over prior work. | relation extraction, semi-structured web, graph+LM | Approach for clean relation harvesting from web tables/pages without domain data. |
| R59 | 2403.07384v2.pdf | Yang Y. et al., “SMALL TO LARGE (S2L): Scalable Data Selection for Fine-tuning Large Language Models,” 2024 | Uses small-model loss trajectories to cluster data and sample subsets with bounded gradient error, cutting fine-tune data needs (e.g., 11% of MathInstruct) while matching accuracy. | data selection, fine-tuning, loss trajectories | Data-efficient recipe for curating fine-tuning corpora for scraper assistants. |
| R60 | 2404.05225.pdfv1.pdf | Luo C. et al., “LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding,” 2024 | Layout-aware pretraining plus supervised tuning and LayoutCoT improve region focus and interpretability for document VQA/extraction, outperforming open 7B baselines. | document understanding, layout, instruction tuning, LayoutCoT | Enhances handling of layout-rich scraped documents. |
| R61 | 2404.05225.pdfv1 (1).pdf | Luo C. et al., “LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding,” 2024 | Duplicate of R60 (file copy). | duplicate | File duplicate of LayoutLLM. |
| R62 | 2404.05225.pdfv1 (2).pdf | Luo C. et al., “LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding,” 2024 | Duplicate of R60 (second copy). | duplicate | Second duplicate file. |
| R63 | 2404.12753.pdfv2.pdf | Huang W. et al., “AUTOSCRAPER: A Progressive Understanding Web Agent for Web Scraper Generation,” 2024 | Duplicate of R16 (alternate filename). | duplicate | Alternate file for AutoScraper. |
| R64 | 2406.10710v2.pdf | Zhong Z. et al., “SyntheT2C: Generating Synthetic Data for Text2Cypher,” 2024 | Builds synthetic natural-language–to–Cypher pairs via LLM prompting and templates (MedT2C), improving Text2Cypher performance on medical KGs after SFT. | Text2Cypher, synthetic data, KG, query generation | Illustrates synthetic data pipelines for NL-to-API translation tasks in scraping. |
| R65 | 2407.05040v1.pdf | Tsai Y.-D. et al., “Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning,” 2024 | Clustering/pruning synthetic code data shows 10% of data retains benchmarks and moderate pruning can improve code quality while reducing compute. | code LLM, data pruning, efficiency | Guides dataset slimming for code-generation components in agents. |
| R66 | 2407.16434.pdfv2.pdf | Liu K. et al., “Enhancing LLM’s Cognition via Structurization,” 2024 | Converts plain context into ordered hierarchical structures to focus attention, improving QA, hallucination evaluation, and retrieval; distilled StruXGPT-7B executes structurization. | context structuring, cognition, QA, hallucination | Method to restructure long scraped contexts before reasoning. |
| R67 | 2408.09434v2.pdf | Narayanan P. et al., “HySem: A Context Length Optimized LLM Pipeline for Unstructured Tabular Extraction,” 2024 | Pipeline with context-length optimization to turn HTML tables into semantic JSON; fine-tuned small model for cost/privacy-sensitive pharma compliance tasks. | table extraction, context optimization, semantic JSON | Practical extraction pipeline for regulated domains with hardware limits. |
| R68 | 2409.02098.pdfv2.pdf | Ziegler I. et al., “CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation,” 2024 | Retrieves relevant web corpus via few-shot similarity then LLM-augments into task-formatted samples; outperforms Self/Evol-Instruct for QA and summarization. | synthetic data, retrieval, augmentation, few-shot | Blueprint for cheap task data generation for scraper/evaluator training. |
| R69 | 2409.19445v1.pdf | Kawamura K., Yamamoto A., “HTML-LSTM: Information Extraction from HTML Tables in Web Pages using Tree-Structured LSTM,” 2024 | Extends tree-structured LSTM to integrate structurally different but semantically similar HTML tables, merging multiple sources for unified retrieval. | HTML tables, tree LSTM, information extraction | Technique for consolidating heterogeneous web tables into one structured dataset. |
| R70 | 2410.09168.pdfv1.pdf | Zhezherau A., Yanockin A., “Hybrid Training Approaches for LLMs: Leveraging Real and Synthetic Data,” 2024 | Combines real transcripts with synthetic persona interactions to fine-tune domain LLMs; hybrid model outperforms real-only and base models across metrics. | hybrid fine-tuning, synthetic data, domain LLM | Evidence for mixing synthetic and real data when adapting scraper assistants. |
| R71 | 2410.12164v1.pdf | Xing J. et al., “Table-LLM-Specialist: Language Model Specialists for Tables using Iterative Generator-Validator Fine-tuning,” 2024 | Generator-validator self-training creates synthetic dual-task data to fine-tune table specialists, achieving GPT-4-level quality without manual labels. | tables, generator-validator, self-training | Method for high-quality table understanding in scraped datasets at low label cost. |
| R72 | 2410.18362v2.pdf | Liang S. et al., “WAFFLE: Fine-tuning Multi-Modal Model for Automated Front-End Development,” 2025 | Structure-aware attention plus contrastive image-HTML alignment boosts UI-to-HTML code generation, improving HTML match and visual similarity on benchmarks. | UI to HTML, multimodal, structure-aware, contrastive | Applicable to agents converting screenshots to executable front-end code. |
| R73 | 2410.18362v2 (1).pdf | Liang S. et al., “WAFFLE: Fine-tuning Multi-Modal Model for Automated Front-End Development,” 2025 | Duplicate of R72. | duplicate | Duplicate WAFFLE file. |
| R74 | 2410.19290.pdfv1.pdf | Liu Y. et al., “Prereq-Tune: Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning,” 2024 | Adds prerequisite knowledge learning before SFT and uses synthetic data to reduce hallucinations, improving factuality on QA and long-form generation. | hallucination reduction, synthetic data, prerequisite learning | Approach to stabilize factual outputs from scraper-driven LLMs. |
| R75 | 2410.22456v1.pdf | Somerville Roberts J. et al., “Image2Struct: Benchmarking Structure Extraction for Vision-Language Models,” 2024 | Automatic benchmark prompting VLMs to generate LaTeX/HTML/music code from images and compare renderings; fresh web/LaTeX/music domains with multiple metrics. | VLM benchmark, structure extraction, HTML, LaTeX | Benchmark for evaluating image-to-structure models relevant to web screenshot parsing. |
| R76 | 2410.22456v1 (1).pdf | Somerville Roberts J. et al., “Image2Struct: Benchmarking Structure Extraction for Vision-Language Models,” 2024 | Duplicate of R75. | duplicate | Duplicate Image2Struct file. |
| R77 | 2412.07958.pdfv2.pdf | Krishna S. et al., “PAFFA: Premeditated Actions For Fast Agents,” 2024 | Builds reusable action library (“Dist-Map”, “Unravel”) to reduce repeated HTML parsing, cutting inference tokens by 87% while keeping step accuracy on web tasks. | web agents, action library, efficiency, tool reuse | Shows inference-time optimization for faster scraping agents. |
| R78 | 2503.01151.pdfv1.pdf | Wang F. et al., “ReaderLM-v2: Small Language Model for HTML to Markdown and JSON,” 2025 | 1.5B model with 512K context converts messy HTML to Markdown/JSON; three-stage data synthesis and multi-objective training outperform larger models on long docs. | HTML conversion, small LM, long context, grounding | Candidate lightweight converter for grounding scraped pages into structured text. |
| R79 | 2506.16594v1.pdf | Rao H. et al., “A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications,” 2025 | PRISMA scoping review of 59 studies (2020–2025) on synthetic biomedical data across modalities, methods (prompting/finetuning), and evaluations. | synthetic data, biomedical, review | Context on privacy-aware synthetic data strategies relevant to sensitive scraping. |
| R80 | 2508.15478v2.pdf | Pham N.T. et al., “SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts—Extended Version,” 2025 | Benchmarks 15 small LMs on accuracy, compute, and sustainability across 9 tasks and 4 hardware setups; open-source pipeline highlights trade-offs. | small LM, benchmark, efficiency, sustainability | Helps select small models for scraper workloads balancing cost and accuracy. |
| R81 | 2510.04871v1.pdf | Jolicoeur-Martineau A., “Less is More: Recursive Reasoning with Tiny Networks,” 2025 | Tiny Recursive Model (7M params) outperforms many LLMs on puzzles (ARC-AGI, Sudoku) using simple recursive reasoning without large data. | tiny model, recursive reasoning, puzzles | Indicates potential of compact reasoning modules for lightweight agents. |
| R82 | 2510.18143v1.pdf | Song H. et al., “Learning from Generalization Patterns: Evaluation-Driven Data Augmentation for Fine-Tuning Small Language Models,” 2025 | PaDA-Agent mines validation failures to design pattern-guided augmentation strategies, improving SLM fine-tuning beyond error-only approaches. | data augmentation, SLM, evaluation-driven | Workflow for iterative augmentation when adapting small scraper models. |
| R83 | 3580305.3599929.pdf | Wang Z. et al., “VRDU: A Benchmark for Visually-rich Document Understanding,” 2023 | Benchmark with diverse schemas, hierarchical fields, and layouts; few-shot and standard settings reveal challenges in template generalization and hierarchy extraction. | document understanding, benchmark, VRDU, hierarchy | Evaluation set for multimodal models handling complex business documents. |
| R84 | 41467_2024_Article_45914.pdf | Polak M., Morgan D., “Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering,” Nat. Commun. 2024 | ChatExtract prompt recipe uses conversational LLM with follow-up questions to identify and verify data sentences, achieving ~90% precision/recall on materials data. | prompt engineering, data extraction, conversational LLM | Demonstrates prompt-only pipeline for accurate data mining from PDFs. |
| R85 | Data-Centric_Fine-Tuning_of_Small_Language_Models_for_Automatic_Extraction_of_Technical_Requirements.pdf | Müller L. et al., “Data-Centric Fine-Tuning of Small Language Models for Automatic Extraction of Technical Requirements,” IEEE Access 2025 | Data-centric prompting enhances training data to fine-tune small LMs for requirement extraction under privacy/cost constraints; evaluates in industrial setting. | requirements extraction, small LM, data-centric, fine-tuning | Shows domain adaptation of compact models for structured requirement scraping. |
| R86 | ExampleProjectFromPreviousYears.pdf | “Mestska aplikacia pod spravou ministerstva – ME100,” FIIT STU student project | Slovak architecture and migration plan for a municipal information system; includes AS-IS/TO-BE views, gaps, schedule, and cost analysis. | case study, architecture, municipal system | Non-research example illustrating system design documentation structure. |
| R87 | futureinternet-16-00167.pdf | Ye A. et al., “A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis,” Future Internet 2024 | Proposes LLM-assisted pipeline for screening and summarizing literature while retaining SR rigor; balances automation with human oversight. | systematic review, workflow, LLM assistance | Relevant for building literature-mining agents with controlled automation. |
| R88 | tacl_a_00466.pdf | Shen Z. et al., “VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups,” TACL 2023 | Inserts layout-group tokens (I-VILA) and hierarchical encoding (H-VILA) to boost PDF token classification with lower training cost and faster inference. | PDF extraction, layout groups, efficiency | Techniques to improve structured extraction from PDF-format scraped documents. |
