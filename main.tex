\documentclass{article}
\usepackage[
    backend=biber,
    style=numeric,
    sorting=none
]{biblatex}

\addbibresource{refs.bib}

\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{subcaption}
\usepackage{todonotes}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}
\doublespacing
\begin{document}
\begin{center}
\thispagestyle{empty}
\textbf{\LARGE Slovak University of Technology in Bratislava}
\par\end{center}{\Large \par}

\begin{center}
\textbf{\Large Faculty of Informatics and Information Technologies}
\par\end{center}{\Large \par}

\vfill{}

\begin{center}

\begin{center}
\textbf{\LARGE Large Language Models as Critics in Multi-Agent Semantic Web Crawling}
\par\end{center}{\huge \par}

\medskip{}

\textbf{\Large Bc. Andrej Vyšný}
\par\end{center}{\Large \par}

\medskip{}

\vfill{}

\textbf{Study program:} Intelligent Software Systems

\textbf{Instructor:} Ing. William Brach

\textbf{Academic year:} 2025 / 2026
\thispagestyle{empty}
\newpage

\thispagestyle{empty}
\newpage

\section{Assignment}
\textbf{
Application of artificial intelligence methods to website scraping problems}

With the popularization of large language models (LLMs), the question has emerged of how to keep these models up to date and ensure that their knowledge reflects current events in the world. Most up-to-date information is found in the online web environment. As the need to provide LLMs with current knowledge grows, so does the need to create, use, and maintain reliable web scrapers that collect the latest information.

The use of artificial intelligence (AI) in web scraping represents a new approach to obtaining data from the internet. Unlike traditional web scraping methods, AI tools provide the ability to dynamically adapt and subsequently process unstructured data. The AI models used in this area are based on machine learning, specifically on natural language processing (NLP), which has laid the foundations for advanced web scraping techniques in various sectors, including e-commerce, market research, and news media.

Frameworks such as PyTorch, Hugging Face, LangChain, and DSPy enable the development of sophisticated AI models and agents for web scraping, while tools like Playwright and Beautiful Soup provide a robust foundation for data extraction, including from dynamic, JavaScript-heavy sites. New tools for preprocessing web pages, such as Jina Reader, FireCrawl, or Crawl4AI, extend the possibilities of web scraping for use in combination with LLMs.

Analyze these technologies and compare their ecosystems from the perspective of both developers and end users. Also compare these tools in terms of their capabilities and accuracy in data extraction. Propose and implement an AI-powered web scraping tool that uses the latest technologies in the field of machine learning. Discuss how this approach differs from traditional web scraping methods. Finally, evaluate the tool you have implemented in terms of accuracy, scalability, and its ability to process dynamic web content from different domains.



\newpage

\section{Related work}

Classical web crawling research has focused on scalable architectures, politeness, and efficient coverage of large link graphs. Early parallel and distributed crawlers introduced partitioning strategies such as domain-based sharding and seed–client architectures to balance load and reduce duplication, typically using link-based or simple lexical heuristics (for example, PageRank-like scores, indegree, or URL patterns) to prioritize pages \cite{gupta2014webparf, mukhopadhyay2011websailor, yadav2012incrementalcrawler, boldi2014bubing, olston2010webcrawling}.

Focused and semantic crawlers concentrate the crawl budget on topic-relevant pages. They rely on natural language processing techniques such as named entity recognition and contextual embeddings to filter and prioritize content, substantially reducing the number of irrelevant documents and improving efficiency compared to purely link-based strategies \cite{dipietro2014semanticcrawling, kuehn2023threatcrawl}. Neural and LLM-guided scoring methods further estimate semantic quality or potential relevance using learned quality estimators, enabling crawlers to visit high-yield pages earlier and to prune low-quality content \cite{pezzuti2025neuralprioritization, kong2024lmds, chang2024neuralpassage}.

In parallel, LLM-based web agents and multi-agent architectures operate directly in real-world web environments. Systems such as DeepResearcher, WebSailor, and WebResearcher train or evaluate LLM agents that use tool calls (for example, web search and browsing) to answer complex queries over the open web, often combining multi-step reasoning, specialized agents that critique each other, and reinforcement learning guided by outcome-based rewards and LLM-as-a-Judge signals \cite{zheng2025deepresearcher, li2025websailor, qiao2025webresearcher}. These systems, however, typically rely on external search APIs or generic browsing interfaces rather than exposing a dedicated focused crawling layer with explicit information retrieval metrics. Practical frameworks such as LangChain, DSPy, FireCrawl, Crawl4AI, and Playwright-based tooling illustrate an emerging ecosystem of LLM-centric web scraping and orchestration libraries; the thesis will draw on these tools when analyzing related work and designing baselines.

The proposed thesis builds on this literature by combining focused and semantic crawling, neural quality estimation, and LLM-based web agents within a single architecture where multi-agent exploration, LLM-based reviewing, and (optionally) reinforcement learning jointly drive crawling, validation, and ranking for bounded-domain RAG applications.

\section{Research target}

The starting point of this thesis is the observation that large language models (LLMs) require timely, high-quality training and retrieval data, while most up-to-date information is scattered across heterogeneous web content such as HTML pages, APIs, and documents. Existing crawling and scraping pipelines prioritize coverage, politeness, and freshness, often guided by link-based or lexical heuristics, whereas recent research has introduced semantic and domain-specific focused crawling and LLM-guided document selection to better target relevant, high-quality content \cite{gupta2014webparf, mukhopadhyay2011websailor, olston2010webcrawling, dipietro2014semanticcrawling, kuehn2023threatcrawl, pezzuti2025neuralprioritization, kong2024lmds, chang2024neuralpassage}.

Despite these advances, crawling, document quality estimation, and retrieval-augmented generation (RAG) are still usually treated as loosely coupled components. Focused crawlers optimize harvest rate and ranking metrics on a static or slowly evolving link graph, while LLM-guided data selection is typically applied offline to an already collected corpus. Deep research agents operate on top of search APIs and ad-hoc browsing, but rarely expose a reusable crawling and indexing layer with explicit information retrieval (IR) metrics and data quality controls \cite{zheng2025deepresearcher, li2025websailor, qiao2025webresearcher}. There is limited empirical evidence on how LLM-based critics, multi-agent exploration, and reinforcement learning (RL) can be combined to improve both the quality of a domain-specific corpus and the performance of RAG systems built on top of it.

The main objective of this thesis is to design, implement, and evaluate a multi-agent, LLM-assisted semantic crawling system that uses LLMs as critics of data quality and relevance. The system will operate in a bounded domain (for example, a focused set of technical or scientific websites) and will act as a retrieval layer for a downstream RAG or deep-research application. Several researcher agents will explore the web in parallel using different strategies, while reviewer agents---implemented as LLM-based critics---score candidate documents for relevance, quality, recency, and usefulness. These scores will drive both the prioritization of further crawling and the ranking of documents in the index. RL will be explored, if feasible, as an optional mechanism for adapting crawling and query strategies based on reviewer feedback and downstream RAG performance.

\subsection{Problem statement and research gap}

Current web crawling pipelines are typically optimized for large-scale indexing and generic search, not for targeted, domain-specific deep research. They rely on homogeneous heuristics for link prioritization and treat document validation as a separate filtering step. At the same time, LLM-based evaluation and LLM-as-a-Judge techniques have shown promise for assessing document quality and answer correctness, but are rarely used as first-class signals in the crawling and ranking loop itself \cite{pezzuti2025neuralprioritization, kong2024lmds}. Multi-agent architectures and RL-based web agents further demonstrate that diverse strategies and outcome-based rewards can improve long-horizon reasoning on the web \cite{zheng2025deepresearcher, li2025websailor, qiao2025webresearcher}, yet they typically operate on top of external search services rather than controlling the underlying focused crawler. This creates a research gap for architectures that explicitly integrate:
\begin{itemize}
	    \item semantic and LLM-aware focused crawling, including neural prioritization and domain-specific relevance estimation;
	    \item LLM-based reviewers that act as critics of document quality and as sources of reward signals for RL policies; and
	    \item multi-agent exploration strategies that trade off breadth, depth, and diversity under a constrained crawl budget.
\end{itemize}
The thesis will investigate whether such an integrated architecture can measurably improve harvest rate and ranking quality during early crawling, as well as the robustness and factual accuracy of downstream RAG answers, compared to strong non-agentic and non-LLM baselines.

\subsection{Main objective}

The main objective of this thesis is to design, implement, and empirically evaluate a reusable multi-agent, LLM-assisted semantic crawling framework for a bounded web domain that serves as a retrieval layer for retrieval-augmented generation (RAG) applications. The framework is intended to be reusable for future research on LLM-based web agents and RAG-aware crawling in similar bounded domains.

\subsection{Research questions}

To achieve this objective, the thesis will address the following research questions and exploratory objectives:
\begin{itemize}
    \item \textbf{RQ1 -- LLM-based versus traditional scoring:} How does LLM-based document quality and relevance scoring compare to traditional and embedding-based scoring for focused crawling and RAG retrieval in terms of crawling efficiency, ranking quality, and downstream answer quality?
    \item \textbf{RQ2 -- Multi-agent researcher and reviewer architecture:} Can a multi-agent setup with multiple researcher and reviewer agents (LLM critics) discover a more diverse and higher-quality set of documents than a single crawler or single-agent baseline under the same resource budget, and how does inter-agent feedback and sharing of high-value hubs influence performance?
	    \item \textbf{Exploratory objective -- RL-guided navigation and strategy adaptation (optional):} Explore whether applying RL over high-level crawling and query actions (for example, link and source selection, depth-versus-breadth trade-offs) improves early harvest rate and RAG performance compared to hand-designed heuristics within the time and computational constraints of the thesis.
	    \item \textbf{RQ3 -- Reliability of LLM-based validation:} How reliable are LLM-as-a-Judge signals (relevance, factuality, recency, usefulness) when used for ranking and, where RL is applied, as rewards compared to human annotations and traditional IR metrics, and under what conditions do these signals introduce bias or instability into the crawling and retrieval pipeline?
\end{itemize}

\subsection{Scope and feasibility}

The thesis will focus on a single, well-defined domain and a limited set of crawling and agent policies to keep the project feasible. Existing LLMs and embedding models will be reused rather than trained from scratch, and the experimental setup will prioritize clear, reproducible metrics and ablation studies over exhaustive exploration of algorithmic variants.

\subsection{Expected contributions}

The expected contributions of this thesis are:
\begin{itemize}
    \item a prototype architecture for multi-agent, LLM-assisted semantic crawling with integrated reviewer agents and optional RL-guided navigation;
	    \item an empirical comparison of LLM-based document scoring against neural quality estimators, embedding-based similarity, and traditional IR baselines within a focused crawling pipeline;
	    \item experimental evidence on the impact of multi-agent exploration and, where implemented, RL-based adaptation on harvest rate, ranking quality, corpus freshness, and downstream RAG answer quality; and
	    \item a reusable experimental framework (code, configuration, and evaluation scripts) that can support future work on deep research agents and RAG-aware crawling.
\end{itemize}

\section{Methodology}

The methodology follows a design-science and experimental approach in which a prototype multi-agent, LLM-assisted semantic crawler is iteratively developed and evaluated against strong baselines on a bounded web domain.

\subsection{Overall research design}

The work is organized into the following phases:
\begin{enumerate}
    \item \textbf{Domain selection and ground truth:} choose a bounded, technically oriented domain (for example, a subset of cyber threat intelligence sources or scientific websites), define information needs and representative queries, and construct a small gold-standard set of manually labeled documents used for hyper-parameter tuning and supervised metrics. Where the domain relies heavily on client-side rendering or dynamic content, incorporate browser-based scraping tools (for example, Playwright) to capture rendered HTML.
    \item \textbf{Baseline crawling and retrieval pipeline:} implement a focused crawler with traditional or embedding-based scoring that collects and parses pages into passages using standard HTML parsing and boilerplate removal, and build a retrieval-augmented generation (RAG) pipeline on top of a vector index to provide the main non-LLM and non-agentic baseline.
    \item \textbf{LLM-based validation and scoring:} design reviewer prompts so that a strong LLM scores documents along relevance, quality, recency, and factuality; combine these into a single quality score and, where feasible, distil them into a cheaper classifier or regressor used during online crawling.
    \item \textbf{Multi-agent architecture:} extend the baseline into a system with several researcher agents that propose crawling actions and reviewer agents that apply LLM-based evaluation, coordinated through a shared frontier and index.
    \item \textbf{RL-guided policy (optional):} if resources allow, model high-level strategy adaptation as a reinforcement learning problem with compact state features summarizing crawler behavior, actions over link and source selection and exploration parameters, and rewards combining reviewer scores, crawling metrics, and RAG performance.
    \item \textbf{Evaluation and ablation studies:} compare system variants under controlled crawl budgets using crawling, ranking, and RAG metrics, and perform ablation studies to isolate the contribution of LLM-based scoring, multi-agent exploration, and any RL-based adaptation.
\end{enumerate}

\section{Evaluation}

Evaluation focuses on three aspects: (i) crawling efficiency and coverage, (ii) retrieval and ranking quality, and (iii) downstream RAG answer quality. All experiments compare the proposed system against well-defined baselines under matched resource budgets and report basic scalability indicators such as how performance and resource usage change with increasing crawl budget or domain size.

\subsection{Evaluation objectives and baselines}

The main evaluation objectives are:
\begin{itemize}
    \item quantify how LLM-based document scoring affects crawling efficiency and ranking quality compared to traditional or embedding-based scoring (RQ1);
    \item measure the impact of the multi-agent architecture on the diversity and quality of discovered documents (RQ2);
    \item evaluate the reliability and stability of LLM-based judgments used as ranking signals and, where RL is applied, as reward signals (RQ3); and
    \item as an exploratory objective, assess whether RL-guided strategies, if implemented, improve early-yield effectiveness and RAG performance over hand-crafted policies.
\end{itemize}
To address these objectives, the following system variants will be compared:
\begin{enumerate}
    \item a \emph{baseline semantic crawler} with embedding-based scoring and no LLM reviewers;
    \item a \emph{single-agent crawler with LLM-based scoring}, where reviewer scores influence frontier prioritization and index filtering;
    \item a \emph{multi-agent crawler} with multiple researcher and reviewer agents sharing a common index and feedback channels; and
    \item optionally, an \emph{RL-enhanced multi-agent system}, where a learned policy adapts high-level strategies.
\end{enumerate}
All configurations use the same domain, seed set, and crawling budget, and they are evaluated on identical query sets and labeled subsets of documents.

\subsection{Crawling metrics}

To evaluate crawling efficiency and coverage, the thesis will use metrics inspired by focused crawling and neural prioritization research \cite{kuehn2023threatcrawl, dipietro2014semanticcrawling, pezzuti2025neuralprioritization}:
\begin{itemize}
    \item \textbf{Harvest rate (HR):} the proportion of fetched documents that are labeled as relevant in the gold-standard set. HR is computed cumulatively as a function of the number of pages fetched, emphasizing early-yield performance.
    \item \textbf{Relevant documents found in budget $B$:} the absolute number of relevant documents discovered within a fixed crawl budget (for example, after $B$ pages), which reflects how efficiently a variant uses its resources.
    \item \textbf{Coverage of sources and topics:} the number of distinct domains or source sites visited and the distribution of topics covered, to assess whether multi-agent strategies improve diversity.
    \item \textbf{Freshness (if applicable):} for domains with time-stamped content, simple freshness indicators (for example, fraction of documents updated within the last year) will be reported, following incremental crawling work \cite{yadav2012incrementalcrawler, jcssp.2015.120.126}.
\end{itemize}
Relevance labels for HR and related quantities are obtained from the manually annotated gold-standard set, possibly complemented by carefully calibrated LLM-based annotations in low-risk cases.

\subsection{Retrieval and ranking metrics}

Retrieval and ranking quality are assessed on a held-out query set with document-level or passage-level relevance labels. The main metrics are:
\begin{itemize}
    \item \textbf{nDCG@k:} normalized discounted cumulative gain at cut-off $k$, which captures both relevance and rank position and is widely used in information retrieval and focused crawling evaluation \cite{pezzuti2025neuralprioritization};
    \item \textbf{MAP@k and MRR:} mean average precision and mean reciprocal rank at cut-off $k$, reflecting early precision and the ability to place at least one relevant item near the top of the ranking; and
    \item \textbf{Recall@k:} the fraction of all relevant documents for a query that appear within the top-$k$ retrieved items, indicating how much of the relevant corpus the system can surface.
\end{itemize}
These metrics are computed separately for the baseline and for each LLM-enhanced variant, using the same underlying annotated subset of the corpus. When sampling documents for labeling, care is taken to include both high-scoring and low-scoring items to obtain meaningful estimates.

\subsection{RAG answer quality}

To assess the impact of crawling and scoring on downstream question answering, a subset of queries is equipped with reference answers or detailed evaluation rubrics. For each system variant:
\begin{itemize}
    \item the RAG pipeline retrieves top-$k$ passages from the corresponding index and generates an answer using a fixed LLM;
    \item automatic overlap-based metrics such as token-level F1 or ROUGE-1 are computed where suitable reference answers exist; and
    \item an LLM-as-a-Judge evaluation is performed using a separate model prompted to rate answers along dimensions such as correctness, relevance, completeness, and faithfulness to the retrieved evidence, following practices from deep research and web-agent work \cite{zheng2025deepresearcher, li2025websailor, qiao2025webresearcher}.
\end{itemize}
For a smaller subset of queries, human judgments are collected (for example, from the author or domain experts) to calibrate the LLM-as-a-Judge scores and to check for systematic biases. Where appropriate, correlations between human and LLM-based ratings are reported.

\subsection{Ablation and robustness analysis}

To better understand the contribution of individual components, the thesis will conduct ablation studies, such as:
\begin{itemize}
    \item disabling LLM-based scoring while keeping the multi-agent architecture, to isolate the effect of reviewers;
    \item collapsing the multi-agent system into a single agent with the same reviewer, to measure the benefit of parallel exploration; and
    \item turning off any RL-based adaptation (if implemented) to compare learned policies to heuristic strategies.
\end{itemize}
Each experiment is repeated with different random seeds or permutations of the crawl frontier, where possible, and mean values with confidence intervals are reported. For key comparisons (for example, baseline versus full system), simple significance tests (such as paired $t$-tests or non-parametric alternatives) are used to assess whether observed improvements are statistically meaningful, within the limitations of thesis-scale experiments.

\printbibliography
\end{document}
