\documentclass{article}
\usepackage[
    backend=biber,
    style=numeric,
    sorting=none
]{biblatex}

\addbibresource{refs.bib}

\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{subcaption}
\usepackage{todonotes}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}
\doublespacing
\begin{document}
\begin{center}
\thispagestyle{empty}
\textbf{\LARGE Slovak University of Technology in Bratislava}
\par\end{center}{\Large \par}

\begin{center}
\textbf{\Large Faculty of Informatics and Information Technologies}
\par\end{center}{\Large \par}

\vfill{}

\begin{center}

\begin{center}
\textbf{\LARGE Optimizing small LLM for HTML data extraction on edge devices}
\par\end{center}{\huge \par}

\medskip{}

\textbf{\Large Bc. Andrej Vyšný}
\par\end{center}{\Large \par}

\medskip{}

\vfill{}

\textbf{Study program:} Intelligent Software Systems

\textbf{Instructor:} Ing. William Brach

\textbf{Academic year:} 2025 / 2026
\thispagestyle{empty}
\newpage

\thispagestyle{empty}
\newpage

\section{Assignment}
\textbf{
Application of artificial intelligence methods to website scraping problems}

With the popularization of large language models (LLMs), the question has emerged of how to keep these models up to date and ensure that their knowledge reflects current events in the world. Most up-to-date information is found in the online web environment. As the need to provide LLMs with current knowledge grows, so does the need to create, use, and maintain reliable web scrapers that collect the latest information.

The use of artificial intelligence (AI) in web scraping represents a new approach to obtaining data from the internet. Unlike traditional web scraping methods, AI tools provide the ability to dynamically adapt and subsequently process unstructured data. The AI models used in this area are based on machine learning, specifically on natural language processing (NLP), which has laid the foundations for advanced web scraping techniques in various sectors, including e-commerce, market research, and news media.

Frameworks such as PyTorch, Hugging Face, LangChain, and DSPy enable the development of sophisticated AI models and agents for web scraping, while tools like Playwright and Beautiful Soup provide a robust foundation for data extraction, including from dynamic, JavaScript-heavy sites. New tools for preprocessing web pages, such as Jina Reader, FireCrawl, or Crawl4AI, extend the possibilities of web scraping for use in combination with LLMs.

Analyze these technologies and compare their ecosystems from the perspective of both developers and end users. Also compare these tools in terms of their capabilities and accuracy in data extraction. Propose and implement an AI-powered web scraping tool that uses the latest technologies in the field of machine learning. Discuss how this approach differs from traditional web scraping methods. Finally, evaluate the tool you have implemented in terms of accuracy, scalability, and its ability to process dynamic web content from different domains.

Within this broad assignment, the thesis will specifically focus on adapting and optimizing small local language models (approximately 7--8 billion parameters) for reliable structured data extraction from HTML pages on resource-constrained edge devices. The following sections refine the research context, goal, and methodology in line with this specialization.

\newpage

\section{Related work}

\subsection{HTML understanding and small LLMs}

Recent work shows that language models can exploit HTML structure directly rather than treating web pages as plain text. HTLM pre-trains on simplified HTML markup to leverage tag and formatting cues, improving classification and summarization of web content \cite{aghajanyan2021htlm}. DOM-LM jointly encodes tokens and DOM-tree structure with specialized position embeddings, yielding strong representations for attribute extraction and question answering over web pages \cite{deng2022domlm}. WebFormer extends this direction by introducing HTML-specific tokens and layout-aware attention for structured field extraction on semi-structured pages \cite{wang2022webformer}. HySem and related work further show how context-length-optimized pipelines can convert HTML tables into semantic JSON using compact models under strict cost and privacy constraints \cite{narayanan2024hysem}. ReaderLM-v2 demonstrates that even a 1.5B-parameter model with a long context window can robustly convert noisy HTML into Markdown and JSON when trained with appropriate synthetic data and multi-objective losses \cite{wang2025readerlmv2}.

General-purpose LLMs have also been evaluated directly on HTML-centric tasks. Gur et al.\ fine-tune pretrained LLMs for HTML element classification, description generation, and autonomous navigation, showing that language understanding transfers effectively to DOM-level reasoning and that moderate-sized models can match or surpass specialized architectures on benchmarks such as MiniWoB \cite{gur2022understandinghtml}. Neveditsin et al.\ benchmark small language models for open attribute--value extraction and show that JSON is a robust output format; careful schema-aware prompting is required to maintain syntactic validity, especially for long or noisy inputs \cite{neveditsin2025structuredoutput}. Together, these works suggest that small LLMs, when paired with structure-aware inputs and robust output formats, can already perform high-quality HTML data extraction.

\subsection{Efficiency and adaptation of small models}

Adapting small models to specialized extraction tasks under tight compute budgets has been studied from both data-centric and model-centric perspectives. Kaddour and Liu fine-tune large teacher models to generate and label synthetic data for downstream small models, achieving significant gains in low-resource settings \cite{kaddour2023syntheticdata}. CRAFT retrieves relevant corpus segments and augments them into task-formatted examples, outperforming generic instruction-tuning pipelines for domain-specific question answering and summarization \cite{ziegler2024craft}. Müller et al.\ show that carefully designed prompting and data-selection heuristics can substantially improve small-model performance for requirement extraction in privacy-sensitive industrial settings, emphasizing that data quality often matters more than raw model size \cite{muller2025datacentric}.

From an efficiency viewpoint, SLM-Bench benchmarks 15 small models across accuracy, compute, and environmental impact, finding that models in the 3--8B range often provide the best trade-off between quality and sustainability for many workloads \cite{pham2025slmbench}. PERP revisits pruning and retraining, showing that re-optimizing only a small fraction of parameters via LoRA-style adapters can recover much of the performance of a dense model at high sparsity levels \cite{zimmer2024perp}. Structurization methods convert long, unstructured inputs into ordered hierarchical formats that are easier for LLMs to process, improving reasoning on complex or lengthy contexts without changing the base architecture \cite{liu2024structurization}. These results indicate that combining structure-aware preprocessing, data-centric adaptation, and parameter-efficient fine-tuning is a promising path for deploying specialized small LLMs on constrained hardware.

\section{Research context and thesis goal}

\subsection{Problem motivation}

Many practical systems rely on up-to-date, structured data extracted from heterogeneous HTML pages, for example in e-commerce analytics, documentation search, or regulatory monitoring. Rule-based scrapers and wrapper-induction systems are brittle and expensive to maintain, while large cloud-hosted LLMs incur monetary costs, latency, and privacy risks and are difficult to deploy offline. Recent HTML-aware and small-model research suggests that compact LLMs can achieve competitive extraction performance when paired with structure-aware preprocessing, synthetic data pipelines, and careful prompting or fine-tuning \cite{deng2022domlm, wang2022webformer, narayanan2024hysem, wang2025readerlmv2, neveditsin2025structuredoutput, muller2025datacentric}. However, there is limited guidance on how to design an end-to-end pipeline that specializes small local models for HTML structure and semantics on edge devices, or on how such models compare to unspecialized small models and large cloud LLMs in terms of extraction quality and resource efficiency.

\subsection{Research goal}

The goal of this thesis is to explore and demonstrate how small local LLMs (approximately 7--8 billion parameters) can be adapted and optimized for reliable data extraction from HTML pages on resource-constrained edge devices. The work will develop and evaluate a structure-aware pipeline and evaluation framework that specializes small models for HTML structure and semantics, and will provide a systematic empirical comparison of optimized local models, their unspecialized variants, and large cloud LLMs, clarifying under which conditions small on-device models are a viable alternative for structured web data extraction in terms of extraction quality and resource efficiency.

\subsection{Research questions}

To realize this goal, the thesis will address the following research questions:
\begin{itemize}
    \item \textbf{RQ1 (pipeline design):} How do structure-aware HTML preprocessing strategies (for example, DOM-based segmentation and tag pruning) affect the extraction quality and robustness of small local LLMs compared to plain-text baselines?
    \item \textbf{RQ2 (adaptation):} How do parameter-efficient fine-tuning, data-centric synthetic augmentation, and quantization influence the quality--efficiency trade-off of 7--8B local models on HTML extraction tasks relative to unspecialized small models?
    \item \textbf{RQ3 (local vs.\ cloud):} Under which conditions (domain, page complexity, schema difficulty, and context length) can optimized local small models match or approach the extraction quality of large cloud LLMs while offering better latency, cost, or environmental characteristics?
    \item \textbf{RQ4 (structured outputs):} How robust are JSON-style structured outputs---in terms of syntactic validity and schema adherence---across different models and HTML complexity levels?
\end{itemize}

\subsection{Scope and contributions}

The thesis will focus on a limited set of representative HTML-centric extraction tasks across one or more domains, each with a clearly specified structured target representation (for example, JSON records, key--value lists, or tabular rows). Existing 7--8B open-source models will be used as local bases and adapted via parameter-efficient methods and lightweight quantization rather than full-model retraining. A small set of strong cloud LLMs will serve as black-box baselines and teachers for synthetic data generation and distillation, with costs kept manageable by limiting the number of calls and models.

Within this scope, the thesis aims to contribute:
\begin{itemize}
    \item a reusable, structure-aware HTML extraction pipeline for small local LLMs, including DOM-based preprocessing and schema-aware prompting;
    \item a parameter-efficient adaptation strategy for 7--8B local models on HTML extraction tasks, combining synthetic data and small-scale fine-tuning \cite{kaddour2023syntheticdata, ziegler2024craft, muller2025datacentric, zimmer2024perp};
    \item a systematic evaluation of local versus cloud LLMs that jointly measures extraction quality, structured-output robustness, latency, memory footprint, and simple compute or energy proxies, inspired by SLM-Bench and related work \cite{pham2025slmbench};
    \item empirical evidence clarifying in which domains and conditions small on-device models are a viable alternative to large cloud LLMs for structured web data extraction.
\end{itemize}

\section{Methodology}

The methodology follows a design-science and experimental approach. A structure-aware HTML extraction pipeline and several small local LLM variants will be designed, implemented, and evaluated against unspecialized local models and large cloud LLMs on shared tasks and datasets.

\subsection{Experimental design}

The concrete experimental setup will be determined after an initial analysis of the problem space and small-scale pilot experiments. The work is expected to cover a small suite of HTML-centric extraction tasks that differ in domain, page structure, and target output format. For each task, the desired structured output (for example, JSON objects, key--value records, or rows in a logical table) will be specified explicitly. HTML pages for the chosen domains will be collected with standard scraping tools such as Playwright-based crawlers, then parsed into DOM trees. Depending on the task, candidate subtrees around relevant content will be extracted and linearized using structure-aware preprocessing strategies inspired by DOM-LM, WebFormer, HySem, and structurization techniques \cite{deng2022domlm, wang2022webformer, narayanan2024hysem, liu2024structurization}. Labeled examples will be created by combining teacher-LM or rule-based annotations with manual corrections and targeted synthetic augmentation \cite{kaddour2023syntheticdata, ziegler2024craft, muller2025datacentric}.

For each task, the thesis will compare three main model configurations:
\begin{itemize}
    \item a strong cloud LLM accessed via API as a zero- or few-shot baseline;
    \item an unspecialized 7--8B local LLM used with generic extraction prompts and minimal preprocessing; and
    \item a specialized 7--8B local LLM adapted with parameter-efficient fine-tuning and, where appropriate, quantization on curated HTML--JSON pairs.
\end{itemize}

\subsection{Evaluation}

Where feasible, models will be evaluated on shared held-out test sets and run on the same edge device to enable fair comparisons, while allowing for task- or hardware-specific adjustments where necessary. The evaluation is expected to focus on three main aspects:
\begin{itemize}
    \item \textbf{Extraction quality:} field-level precision, recall, and F1; exact-match or schema-level accuracy for structured outputs; and, where relevant, simple domain-specific measures (for example, numeric error on prices or date fields).
    \item \textbf{Structured-output robustness:} fraction of outputs that are syntactically valid and conform to the expected schema or type constraints (for example, valid JSON with required fields), with error categories inspired by prior work on structured-output robustness \cite{neveditsin2025structuredoutput}.
    \item \textbf{Efficiency and resource usage:} latency per page, throughput under batch workloads, memory footprint (including adapter and quantization configurations), and simple compute or energy proxies following SLM-Bench-style reporting \cite{pham2025slmbench}; the exact choice of efficiency indicators may be refined based on profiling during development.
\end{itemize}

Simple ablation experiments will remove individual components, such as structure-aware preprocessing or fine-tuning, to isolate their contributions to quality and efficiency. The exact set of ablations will be selected once the most influential components are identified during experimentation. Where practical, results will be averaged over multiple runs or seeds, and basic confidence intervals will be reported for key comparisons.

\printbibliography
\end{document}
