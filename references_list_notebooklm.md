Na základe celého obsahu, ktorý bol k dispozícii v jednotlivých dokumentoch, nasleduje tabuľka so súhrnmi. Ak dokument obsahoval len referencie, alebo neposkytoval dostatok kontextu na vytvorenie súhrnu, jeho zhrnutie je ponechané prázdne, no napriek tomu je zahrnutý v tabuľke.

| Názov súboru | Prehľad obsahu dokumentu |
| :--- | :--- |
| **1406.5690v1.pdf** | Prázdny (Obsahuje len referencie). |
| **1601.06919v1.pdf** | Dokument analyzuje štruktúru webového grafu na základe snapshotov (napr. uk-2014, eu-2015) a skúma distribúciu prichádzajúcich odkazov (indegree), pričom zistil, že táto distribúcia často nie je power-law (zákon moci). Práca tiež popisuje architektúru crawlovacieho systému, ktorá využíva moderné bez-zámkové dátové štruktúry a dátovú štruktúru "workbench" na efektívne poskytovanie nasledujúcej URL pri dodržiavaní pravidiel zdvorilosti (politeness). |
| **2201.03916v2.pdf** | Ide o rozsiahly prehľad (Survey) v oblasti **Automatizovaného Posilňovaného Učenia (AutoRL)**. Dokument pokrýva širokú škálu techník vrátane Hindsight Experience Replay (HER), distribuovaného RL (DDRL), metód na ladenie hyperparametrov (ako je Bayesovská optimalizácia a Population Based Training), rôzne optimalizačné algoritmy pre hlboké neurónové siete a štúdie zamerané na hlboké RL (napr. AlphaGo a Dota 2). |
| **2209.12006v2.pdf** | Prázdny (Obsahuje len poďakovania a referencie). |
| **2210.03945v2.pdf** | Dokument sa zaoberá výkonom agentov pre navigáciu na webe, konkrétne ich úspešnosťou v úlohách **MiniWoB**. Porovnáva úspešnosť rôznych modelov (napr. CC-Net, DOM-Q-Net, WebN-T5-3B) v úlohách ako `navigate-tree`, `search-engine` a `scroll-text`. Uvádza, že DOM-Q-Net je model založený na RL uzemnený na štruktúrovanom jazyku (DOM). |
| **2212.00253v1.pdf** | Práca sa venuje **Distribuovanému Hĺbkovému Posilňovanému Učeniu (DDRL)**, ktoré preukázalo potenciál v trénovaní veľmi úspešných agentov, ako sú Suphx, OpenAI Five a AlphaStar. Spomína tiež výskum v oblasti akcelerovaných metód DRL a algoritmov Proximal Policy Optimization (PPO). |
| **2304.11960v4.pdf** | Predstavuje **ThreatCrawl**, nový **fokusovaný prehľadávač (focused crawler)** pre **Cyber Threat Intelligence (CTI)**. Využíva **BERT-založené kontextualizované vloženia dokumentov** na dynamické určenie relevancie dokumentu pred extrakciou odkazov, čím dosahuje efektívnu mieru zberu 52% a šetrí zdroje tým, že ignoruje nerelevantné cesty prehľadávania. Dokumenty sú radené na základe ich relatívnej vzdialenosti od vektora pravdivosti. |
| **2305.05027v2.pdf** | Prázdny (Obsahuje len referencie). |
| **2308.07107v2.pdf** | Dokument sumarizuje úlohu a metódy **veľkých jazykových modelov (LLMs) v systémoch na získavanie informácií (IR)**. Popisuje techniky vylepšenia vyhľadávania, ako je preformulovanie dopytov (query reformulation, napr. HyDE), a metódy rerankingu. Tiež predstavuje rôzne prístupy k využitiu LLM pre relevančné úlohy (pointwise, pairwise ranking) a spomína súvisiace oblasti ako **RAG** (Retrieval-Augmented Generation) a škálovanie LLMs (napr. Gopher, GLaM). |
| **2312.03863v2.pdf** | Komplexný **prehľad (Survey) zameraný na efektívne Veľké jazykové modely (LLMs)**. Systematicky triedi techniky efektivity, vrátane **kvantizácie** a **prerezávania** (pruning) modelov (napr. Sheared LLaMA), **efektívneho jemného doladenia** (PEFT ako LoRA), **efektívnej inferencie** (Speculative Decoding, KV-Cache Optimization) a **architektonického dizajnu** (napr. MoE). Rieši aj škálovanie kontextu pomocou kompresie promptov (LongLLMLingua). |
| **2404.07738v2.pdf** | Dokument predstavuje **ResearchAgent**, autonómneho agenta poháňaného LLM, navrhnutého na **urýchlenie vedeckého výskumu a generovanie nových výskumných nápadov**. Agent využíva **entity-centrické úložisko znalostí** a **citačný graf** na prehľadávanie literatúry a hľadanie netriviálnych súvislostí medzi vedeckými konceptmi (napr. spájanie Drosophila a CRISPR). Umožňuje generovanie kompletných výskumných návrhov. |
| **2404.09770v1.pdf** | Dokument sa venuje **zlepšeniu metodológie pre dlhodobú (longitudinal) webovú analýzu pomocou archívu Common Crawl (CC)**. Analyzuje **reprezentatívnosť segmentov** CC a štruktúru archívu (URI indexy, WARC/WET súbory). Práca tiež identifikuje a koriguje anomálie v historických dátach, napríklad neobvyklý nárast hlavičiek `Last-Modified` v roku 2005. |
| **2404.12753v2.pdf** | Dokument predstavuje **AutoScraper**, hybridný systém na **extrakciu štruktúrovaných informácií z webových stránok**. LLM agent generuje **vykonateľné sekvencie akcií (XPaths)** pomocou iteratívneho procesu **Top-down a Step-back**. Systém zahŕňa modul **Synthesis** na výber najlepšej sekvencie pre opätovné použitie na rovnakej webovej stránke. Zistilo sa, že výkon otvorených LLM je obmedzený ich schopnosťou chápať hierarchickú štruktúru stránok. |
| **2405.14779v1.pdf** | Práca navrhuje **inteligentný bilingválny fokusovaný prehľadávač (focused crawler)** zameraný na **zber paralelných textových dát** (prekladov). Kľúčová inovácia spočíva v použití neurónových modelov (založených na XLM-RoBERTa), ktoré dokážu určiť **jazyk dokumentu a paralelnosť dvoch URL adries len na základe samotných URL**, čím sa znižuje potreba sťahovania irelevantného obsahu a urýchľuje zber dát. |
| **2405.18687v1.pdf** | Prázdny (Obsahuje len referencie). |
| **2406.04638v1.pdf** | Dokument predstavuje **LMDS (LLM Guided Web-Crawl Document Selection) pipeline**, škálovateľný rámec, ktorý využíva LLM na **označovanie a výber dokumentov vysokej kvality a vzdelávacej hodnoty** z webových korpusov. Značkovací LLM vyhodnocuje dokumenty, a výsledné dáta sa destilujú do menšieho klasifikátora (`LMsmall`) pre masové filtrovanie. Modely trénované na dátach vybraných pomocou LMDS dosahujú **významne vyššiu kvalitu výkonu** (napr. zlepšenie MMLU skóre). |
| **2406.08246v1.pdf** | Práca popisuje systém využívajúci **Retrieval-Augmented Generation (RAG)** pre extrakciu informácií z HTML obsahu. Systém najprv vyhľadá **top k sémanticky relevantných fragmentov** (chunks) HTML k extrahovanej oblasti. Tieto fragmenty sú spojené a LLM je následne vedený špeciálne navrhnutými promptmi na **extrakciu špecifických údajov** z tohto kombinovaného obsahu. |
| **2407.07630v1.pdf** | Prázdny (Obsahuje len referencie na LLM, ich benchmarking, škálovanie a analýzu kvality multilinguálnych web-crawled datasetov). |
| **2407.12170v1.pdf** | Práca sa zaoberá **neurálnym odhadom kvality pasáží** nezávislým od dopytu s cieľom **statického prerezávania (static pruning) korpusov** pre vyhľadávacie stroje. Ukazuje, že neurónové metódy trénované na relevancii môžu poskytnúť silné signály kvality, ktoré umožňujú odstrániť rozsiahle časti nízko-kvalitného obsahu, čím sa šetria náklady na indexovanie, ukladanie a vyhľadávanie. |
| **2409.15441v1.pdf** | Dokument definuje štruktúru promptu a požadovaný výstup pre agenta, ktorý má za úlohu poskytnúť **jednovetný, vysoko-úrovňový súhrn primárneho účelu a kontextu** danej webovej stránky ("Webpage Context Summary"). |
| **2412.03398v1.pdf** | Dokument predstavuje **REDSTONE**, komplexný *pipeline* navrhnutý na **efektívnu extrakciu a filtrovanie rozsiahlych, rôznorodých dátových súborov z Common Crawl (CC)**. Cieľom je získať dáta vysokej kvality, vrátane špecializovaných domén ako **kód, matematika a Open-Domain QA**. Pipeline využíva viacstupňové filtrovanie (rule-based a model-based) na odstránenie nízko-kvalitného obsahu. |
| **2502.09913v1.pdf** | Prázdny (Obsahuje len referencie). |
| **2502.10200v1.pdf** | Prázdny (Obsahuje najmä referencie týkajúce sa Posilňovaného Učenia (RL), neurónových sietí a chaotických systémov (Chaotic Neural Networks)). |
| **2503.09223v1.pdf** | Prázdny (Obsahuje referencie na rôzne témy súvisiace s LLM, vrátane PEFT, RLHF a Self-Reflection v agentoch). |
| **2503.18102v1.pdf** | Dokument predstavuje **AgentRxiv**, rámec pre **Kolaboratívny Autonómny Výskum**, kde agenti zdieľajú objavy prostredníctvom centralizovaného preprint servera, čím dosahujú **kumulatívne budovanie znalostí** a urýchľujú vedecký pokrok. Dokument tiež popisuje techniku **Simultaneous Divergence Averaging (SDA)**, ktorá zlepšuje presnosť uvažovania generovaním a porovnávaním dvoch Chain-of-Thought odpovedí. |
| **2503.23350v1.pdf** | Prázdny (Obsahuje hlavne referencie na LLM agentov a s nimi súvisiace témy, ako je autonómna navigácia na webe (Agent-E, Mind2Web), bezpečnosť AI agentov, dlho-horizontálne plánovanie (PLAN-AND-ACT) a agenti pre ovládanie operačných systémov (OS-Copilot)). |
| **2504.03160v4.pdf** | Prázdny (Obsahuje referencie na rôzne aspekty LLM a RAG systémov. Obsahuje aj špecifický prompt pre LLM na vykonávanie štruktúrovaného výskumného plánu pre Question Answering pomocou webového prehľadávania). |
| **2504.06219v2.pdf** | Dokument sa zameriava na **kvalitu a súlad (compliance) tréningových dát pre LLM**, špecificky na **filtrovanie dát z webu** na základe ich **tématických a formátových domén**. Dokument analyzuje percentá odstránených dokumentov podľa domény (napr. News Article, Structured Data) a poukazuje na výzvy spojené s autorskými právami a súhlasom s použitím dát (AI data commons). |
| **2504.11011v1.pdf** | Dokument rozširuje použitie neurónových odhadov kvality pasáží na **prioritizáciu webových stránok v procese prehľadávania (crawling prioritisation)**. Navrhuje modul pre **bodovanie sémantickej kvality** (pomocou QT5-small), ktorý sa nasadzuje v **Docker kontajneri** pre ľahkú integráciu do existujúcich vyhľadávacích systémov (napr. OWS). |
| **2506.16146v2.pdf** | Dokument sa zaoberá **Neurálnou Prioritizáciou pre Webové Prehľadávanie**. Zameriava sa na využitie neurónových metód pre efektívny výber a radenie obsahu pri prehľadávaní webu. |
| **2507.02592v1.pdf** | Práca sa týka LLM agenta (pravdepodobne WebSailor), navrhnutého na **autonómne vyhľadávanie informácií** na webe. Agent používa nástroje `Search` a `Visit`, pričom nástroj `Visit` využíva súhrnný model (Qwen-2.5-72B) na extrakciu informácií na základe cieľa. Práca je zameraná na tréning robustných agentov, pravdepodobne pomocou RL a syntetických dát. |
| **2508.19828v4.pdf** | Dokument predstavuje **Memory-R1**, metódu na zlepšenie schopností **LLM agentov manažovať a využívať pamäť** prostredníctvom **Posilňovaného Učenia (RL)**. Cieľom je posilniť schopnosti agenta pri dlho-horizontálnych úlohách a efektívne spracovávať kontext. |
| **2509.13305v1.pdf** | Práca pravdepodobne predstavuje **WebSailor-V2**, LLM agenta, ktorý demonštruje schopnosť riešiť **komplexné, dlho-horizontálne výskumné úlohy** na webe. Poskytuje detailný **sled krokov agenta** (`think`, `search`, `visit`), ktorý vykonáva multi-krokové prehľadávanie a extrakciu špecifických finančných a právnych informácií z reálnych dokumentov (napr. SEC filings, PDF ročné správy) [381–388]. |
| **2509.13309v2.pdf** | Dokument opisuje **trojstupňový workflow syntézy dát** riadený **multi-agentovým systémom** na generovanie komplexných a fakticky správnych **Question Answering (QA) dátových sád**. Worklow zahŕňa **Summary Agent** a **ItemWriter Agent** na iteratívne zvyšovanie komplexnosti úloh, s následným viacstupňovým kontrolným procesom. |
| **2509.13310v1.pdf** | Prázdny (Obsahuje najmä referencie na pokročilé LLM agenty a výskum zameraný na **rozsiahle posilňované učenie (RL)** pre dlho-horizontálne webové úlohy, **používanie nástrojov** (tool learning) a **kontinuálny pre-tréning**). |
| **2510.24698v1.pdf** | Dokument sa zameriava na **optimalizáciu hlbokých agentov pre vyhľadávanie informácií (Deep IS Agents)** prostredníctvom riadeného uvažovania. Metóda dosahuje **výrazné zlepšenie výkonu** (až o 62%) pri súčasnom **znížení nákladov na prieskumné tokeny**. Prístup využíva **syntetické dáta a škálovateľné RL** na zlepšenie schopností agentov. |
| **2510.24699v1.pdf** | Prázdny (Obsahuje referencie na pokročilé agentic modely (Gemini 2.5), LLM web agentov trénovaných pomocou RL a syntetických dát (WebSailor-V2), agenty pre ovládanie GUI/UI trénované pomocou multi-turn RL (UI-TARS-2) a agentickú pamäť (A-Mem)). |
| **3637528.3671620.pdf** | Dokument predstavuje **AutoWebGLM**, agenta na **navigáciu na webe (Web Navigating Agent)** založeného na Veľkých jazykových modeloch (LLM). Agent je trénovaný, aby generoval príkazy a uvažoval s cieľom interakcie s webovými stránkami a dokončenia úlohy. |
| **Semantic\_crawling\_An\_approach\_based\_on\_Named\_Entity\_Recognition.pdf** | Dokument popisuje **Semantic Crawler**, fokusovaný webový prehľadávač (focused web crawler) využívajúci **Named Entity Recognition (NER)**. Kľúčová funkcia spočíva v riešení **sémantickej nejednoznačnosti kľúčových slov** a zabezpečuje, aby sa prehľadávali len dokumenty, kde je kľúčové slovo klasifikované s očakávanou entitou. |
| **jcssp.2015.120.126.pdf** | Práca navrhuje **škálovateľný fokusovaný prehľadávač (focused crawler) s inkrementálnym paralelným webovým prehľadávačom**. Architektúra je navrhnutá na súbežné prehľadávanie stránok relevantných pre **viaceré preddefinované témy** a obsahuje nový model radenia, ktorý zabezpečuje **vysokú kvalitu, relevantnosť a aktuálnosť** (freshness) stiahnutých stránok. |
| **2025.acl-srw.19.pdf** | Evaluates small LMs formatting attribute-value extractions from clinical notes; JSON parses best, while targeted prompting improves robustness but long inputs hurt structure. |
| **2107.06955.pdfv1.pdf** | HTLM pretrains on simplified HTML with denoising, leveraging markup semantics to beat text-only LMs on zero-shot prompting and summarization. |
| **2201.10608.pdfv1.pdf** | DOM-LM jointly encodes text and DOM trees for webpages, delivering better attribute extraction, OIE, and QA generalization. |
| **2201.10608.pdfv1 (1).pdf** | Duplicate copy of DOM-LM; same HTML+DOM representation learning approach. |
| **2202.00217.pdfv1.pdf** | WebFormer introduces HTML tokens with graph attention and layout-aware patterns for structured field extraction, outperforming prior baselines. |
| **2202.00217.pdfv1 (1).pdf** | Duplicate of WebFormer; same transformer architecture for structured web extraction. |
| **2210.03347v2.pdf** | Pix2Struct pretrains an image-to-text model to emit simplified HTML from masked screenshots, reaching SOTA across doc, UI, and vision-language tasks. |
| **2307.12856v4.pdf** | WebAgent combines planning, long-HTML summarization, and code execution (Flan-U-PaLM + HTML-T5) to boost real-site task success. |
| **2309.11042.pdfv1.pdf** | ALTER adds mixture-of-task adapters to small LMs with two-stage training, enabling efficient multi-task learning. |
| **2310.01119.pdfv2.pdf** | Uses fine-tuned teacher LLMs to synthesize/label data, markedly improving small-model performance in low-resource classification and generation. |
| **2312.15230v3.pdf** | PERP shows retraining tiny parameter subsets (LoRA-like) can recover pruned LLMs; introduces mergeable adapters for high-sparsity cases. |
| **2402.04437.pdfv5.pdf** | Defines AESOP metric and MuSEE multi-stage model for entity-centric structured extraction, outperforming baselines. |
| **2402.14129.pdfv1.pdf** | GraphScholarBERT fuses graph and language encoders for zero-shot relation extraction on semi-structured web pages. |
| **2403.07384v2.pdf** | S2L clusters small-model loss trajectories to select fine-tuning subsets, matching full-data performance with far less data. |
| **2404.05225.pdfv1.pdf** | LayoutLLM uses layout-aware instruction tuning and LayoutCoT to improve document VQA/extraction. |
| **2404.05225.pdfv1 (1).pdf** | Duplicate of LayoutLLM; same layout-aware document understanding method. |
| **2404.05225.pdfv1 (2).pdf** | Second duplicate copy of LayoutLLM paper. |
| **2404.12753.pdfv2.pdf** | Duplicate of AutoScraper paper; progressive understanding agent generating reusable web scrapers. |
| **2406.10710v2.pdf** | SyntheT2C builds synthetic NL-to-Cypher pairs (MedT2C) via prompting/templates to improve Text2Cypher on medical KGs. |
| **2407.05040v1.pdf** | Shows clustering/pruning synthetic code data lets code LLM fine-tuning keep quality using only ~10% of data. |
| **2407.16434.pdfv2.pdf** | Structurizes long contexts into hierarchical elements, boosting QA, hallucination checks, and retrieval; distilled StruXGPT-7B performs the structurization. |
| **2408.09434v2.pdf** | HySem pipeline optimizes context length to convert HTML tables into semantic JSON for pharma compliance on modest hardware. |
| **2409.02098.pdfv2.pdf** | CRAFT retrieves similar web text and LLM-augments it into task-formatted samples, outperforming other synthetic data generators. |
| **2409.19445v1.pdf** | HTML-LSTM applies tree-structured LSTM to merge structurally different but semantically similar HTML tables for unified extraction. |
| **2410.09168.pdfv1.pdf** | Hybrid fine-tuning mixing real transcripts and synthetic scenarios yields better domain-specific LLMs than real-only data. |
| **2410.12164v1.pdf** | Table-LLM-Specialist iteratively generator-validates synthetic data to create table specialists reaching GPT-4-like quality without labels. |
| **2410.18362v2.pdf** | WAFFLE adds structure-aware attention and contrastive image-HTML alignment to improve UI-to-HTML code generation metrics. |
| **2410.18362v2 (1).pdf** | Duplicate copy of WAFFLE UI-to-HTML fine-tuning paper. |
| **2410.19290.pdfv1.pdf** | Prereq-Tune inserts prerequisite learning plus fictitious synthetic data to cut hallucinations in QA and long-form generation. |
| **2410.22456v1.pdf** | Image2Struct benchmark evaluates VLMs that generate HTML/LaTeX/music code from images via render-and-compare metrics. |
| **2410.22456v1 (1).pdf** | Duplicate copy of Image2Struct benchmark. |
| **2412.07958.pdfv2.pdf** | PAFFA reuses precomputed action libraries (“Dist-Map”, “Unravel”) to speed web agents, reducing inference tokens by ~87%. |
| **2503.01151.pdfv1.pdf** | ReaderLM-v2 (1.5B, 512K context) converts noisy HTML to Markdown/JSON via three-stage data synthesis, beating larger models on long docs. |
| **2506.16594v1.pdf** | Scoping review (2020–2025) of synthetic biomedical data generation methods, modalities, and evaluations. |
| **2508.15478v2.pdf** | SLM-Bench compares 15 small LMs on accuracy, compute, and sustainability across 9 tasks and 4 hardware setups. |
| **2510.04871v1.pdf** | Tiny Recursive Model (7M) achieves strong puzzle reasoning (ARC-AGI, Sudoku) via simple recursive loops, surpassing many LLMs. |
| **2510.18143v1.pdf** | PaDA-Agent mines validation failures to design pattern-guided augmentations, improving small LM fine-tuning. |
| **3580305.3599929.pdf** | VRDU benchmark features diverse templates and hierarchical fields; highlights difficulty of generalizing document extraction across layouts. |
| **41467_2024_Article_45914.pdf** | ChatExtract prompt framework uses conversational LLM follow-ups to accurately extract materials data (~90% precision/recall). |
| **Data-Centric_Fine-Tuning_of_Small_Language_Models_for_Automatic_Extraction_of_Technical_Requirements.pdf** | Data-centric prompting enhances training data to fine-tune small LMs for technical requirement extraction under privacy and cost constraints. |
| **ExampleProjectFromPreviousYears.pdf** | Slovak student project describing architecture, AS-IS/TO-BE models, migration plan, and costs for a municipal information system. |
| **futureinternet-16-00167.pdf** | Presents LLM-assisted but human-supervised workflow for systematic/literature reviews to reduce screening and summarization effort. |
| **tacl_a_00466.pdf** | VILA inserts layout-group tokens and hierarchical encoding to improve PDF structured extraction with lower cost and faster inference. |
